---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

  My full publication list can be found on <u><a href="https://scholar.google.com/citations?user=7ZNoHJkAAAAJ&hl=en">my Google Scholar profile</a>.</u>

<!-- {% include base_path %} -->
<!-- *: corresponding author -->

## Preprint

### [RL x LLM] Reinforcement Learning in the Era of LLMs: What is Essential? What is Needed? <a href="https://arxiv.org/pdf/2310.06147.pdf"> [Paper] </a>

*__Hao Sun__*

- (1) RLHF is online IRL rather than offline RL. (2) RLHF is better than SFT because imitation learning alleviates the compounding error problem. (3) Insight of RM can be generalized to other LLM applications except alignment. (4) RLHF is more challenging than conventional IRL due to action space dimensionality and reward sparsity. (5) The superiority of PPO in RLHF may originate from its stability.

### [RL x LLM] Query-Dependent Prompt Evaluation and Optimization with Inverse RL <a href="https://arxiv.org/pdf/2309.06553.pdf"> [Paper] </a><a href="https://github.com/holarissun/Prompt-OIRL"> [Code] </a>

*__Hao Sun__, Alihan H端y端k, Mihaela van der Schaar*

- We propose Prompt-OIRL, showing that Inverse RL can be used for offline query-dependent prompt evaluation and optimization. It does not require interactions with the LLMs during learning yet achieves superior performance on arithmetic reasoning tasks.



## Conference


### [NeurIPS 2023] Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples <a href="https://arxiv.org/pdf/2310.07747.pdf"> [Paper] </a><a href=" "> [Code (Soon)] </a>

*__Hao Sun__, Alihan H端y端k, Daniel Jarrett, Mihaela van der Schaar*

- We introduce an effective algorithm to enhance interpretability and accountability in offline RL. This research is critical for responsibility-sensitive applications like finance and healthcare. 


### [NeurIPS 2023] DAUC: a Density-based Approach for Uncertainty Categorization  <a href="https://arxiv.org/pdf/2207.05161.pdf"> [Paper (To Be Updated)] </a><a href="https://anonymous.4open.science/r/DAUX-CBBF"> [Code (Soon)] </a>

*__Hao Sun__ ^, Boris van Breugel^, Jonathan Crabbe, Nabeel Seedat, Mihaela van der Schaar*

- We propose a density-based approach to classify and explain the source of uncertainty.


### [NeurIPS 2022] Exploiting Reward Shifting in Value-Based DRL  <a href="https://arxiv.org/pdf/2209.07288.pdf"> [Paper] </a><a href="https://github.com/2Groza/RewardShifting"> [Code] </a>

*__Hao Sun__, Lei Han, Rui Yang, Xiaoteng Ma, Jian Guo, Bolei Zhou*

- A positive reward shifting leads to conservative exploitation, while a negative reward shifting leads to curiosity-driven exploration.


### [ICLR 2022] Rethinking Goal-conditioned Supervised Learning and Its Connection to Offline RL <a href="https://arxiv.org/abs/2202.04478"> [Paper] </a><a href="https://github.com/YangRui2015/AWGCSL"> [Code] </a>

*Rui Yang, Yiming Lu, Wenzhe Li, __Hao Sun__, Meng Fang, Yali Du, Xiu Li, Lei Han, Chongjie Zhang*
- We optimize the GCSL with a lower bound of the goal-reaching objective and link the success of GCSL from perspective of offline RL.

### [IJCAI 2021] Hierarchical Multi-Scale Gaussian Transformer for Stock Movement Prediction <a href="https://www.ijcai.org/proceedings/2020/0640.pdf"> [Paper] </a>

*Qianggang Ding, Sifan Wu, __Hao Sun__, Jiadong Guo, Jian Guo*

- We adapt transformer to stock movement predictions.

### [AAAI 2021] Adaptive Regularization of Labels <a href="https://arxiv.org/abs/1908.05474"> [Paper] </a>

*Qianggang Ding, Sifan Wu, __Hao Sun__, Jiadong Guo, Shu-Tao Xia*
- We study the correlations between lables to improve model performance.


### [NeurIPS 2019 (Spotlight)] Policy Continuation with Hindsight Inverse Dynamics <a href="https://arxiv.org/abs/1910.14055"> [Paper] </a><a href="https://github.com/2Groza/PCHID_code"> [Code] </a> <a href='https://sites.google.com/view/neurips2019pchid/'> [Homepage] </a>

*__Hao Sun__, Zhizhong Li, Xiaotong Liu, Dahua Lin, Bolei Zhou*
- Supervised Learning can be used to solve goal-conditioned RL tasks.


