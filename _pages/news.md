---
layout: archive
title: "News!"
permalink: /news/
author_profile: true
redirect_from:
  - /news
---

ðŸ“„ (2025.02) Our Reward Model Papers [Part II: Active Reward Modeling](https://arxiv.org/pdf/2502.04354) and [Part III: Infra](https://arxiv.org/pdf/2502.04357) are online. <br> 

ðŸ“„ (2025.01) Our Reward Model Paper [Part I: Foundation, Theory, and Alternatives](https://arxiv.org/pdf/2411.04991) is accepted by ICLR as an **Oral**! <br>

ðŸ’¬ (2024.12) Our [**Tutorial: Inverse RL Meets LLMs**](https://sites.google.com/view/irl-llm) has been accepted by ACL-2025, see you at Vienna!<be>

ðŸ’¬ (2024.10) Our [**Tutorial: Inverse RL Meets LLMs**](https://sites.google.com/view/irl-llm) has been accepted by AAAI-2025 --- Join us in Philadelphia and let us explore the potential of Inverse RL in the era of LLMs! <be>

ðŸ’¬ (2024.10) New talk on **Inverse RL Meets LLMs** at the vdsLab2024 OpenHouse and UCLA Zhou Lab. [Slide](https://holarissun.github.io/files/IRL_LLM_Oct.pdf) is online<be>

ðŸ“„ (2024.09) Our [Data Centric Reward Modeling](https://openreview.net/forum?id=wg5y4AK6l7) paper is accepted by the Journal of Data-Centric Machine Learning Research (DMLR). <be>

ðŸ“„ (2024.08) [InverseRLignment](https://openreview.net/pdf/97e8ef1506b4477fd9dc41a76ea3257f65c66c5e.pdf) is presented at the RL beyond reward workshop (accepted with score 9) at the 1-st RLConference, it **builds reward models from SFT data**.. <be>

ðŸ“„ (2024.05) Our [RLHF with Dense Reward](https://arxiv.org/pdf/2402.00782.pdf) paper is accepted by ICML 2024. <be>

ðŸ’¬ (2024.03) **Prompt-OIRL** and **RATP** are featured at the [Inspiration Exchange](https://www.vanderschaar-lab.com/engagement-sessions/inspiration-exchange/), recording is <a href="https://www.youtube.com/watch?v=NYYYbQ_EN30&ab_channel=vanderSchaarLab"> online </a>. <be>

ðŸ“„ (2024.01) **1 RL + LLM Reasoning paper** is accepted by ICLR 2024! [Prompt-OIRL](https://arxiv.org/pdf/2309.06553.pdf) uses Inverse RL to evaluate and optimize prompts for **Math Reasoning**.<be>

ðŸ’¬ (2024.01) Invited talk on **RLHF** at the [Intuit AI Research Forum](https://www.intuit.com/technology/). <a href="https://holarissun.github.io/files/RLHF_Dec.pdf"> slide </a> <be>

ðŸ’¬ (2023.12) Invited talk on **RLHF** at the [Likelihood Lab](http://www.maxlikelihood.cn/) <a href="https://holarissun.github.io/files/RLHF_Dec.pdf"> slide </a> <be>

ðŸ’¬ (2023.11) Invited talk on **RLHF** at the [CoAI group, THU.](https://huggingface.co/thu-coai). <a href="https://holarissun.github.io/files/RLHF_Nov.pdf"> slide  </a> <be>

ðŸ“„ (2023.10) [Prompt-OIRL](https://arxiv.org/pdf/2309.06553.pdf) is selected as an **oral presentation** at the NeurIPS 2023 ENLSP workshop!<be>

ðŸ“„ (2023.10) I wrote <a href="https://arxiv.org/abs/2310.06147">an article </a> on **RLHF** to share my thoughts as an RL researcher in the Era of LLMs. <be>

ðŸ“„ (2023.09) **2 papers** on [Interpretable Offline RL](https://arxiv.org/abs/2310.07747) and [Interpretable Uncertainty Quantification](https://arxiv.org/abs/2207.05161) are accepted by NeurIPS 2023. <be>

ðŸ’¬ (2023.9) Invited talk on **"Reinforcement Learning in the Era of LLMs"** at [Kuaishou Research](https://usrdc.kuaishou.com/). <a href="https://holarissun.github.io/files/RLHF_Kuai_final.pdf"> slide is online </a>  <be>

ðŸ“„ (2023.2) **2 papers** are accepted by AISTATS 2023. <be>

ðŸ’¬ (2022.11) Invited talk on value-based DRL at HW Cloud Research. <a href="https://sites.google.com/view/rewardshaping"> slide is online </a>  <be>

ðŸ“„ (2022.9) **1 paper** on [Value-Based DeepRL](https://proceedings.neurips.cc/paper_files/paper/2022/file/f600d1a3f6a63f782680031f3ce241a7-Paper-Conference.pdf) is accepted by NeurIPS 2022. 2 papers are presented at the FMDM workshop, and 2 papers are presented at the DeepRL workshop. <be>

ðŸ“„ (2022.1) **1 paper** on [Offline GCRL](https://arxiv.org/abs/2202.04478) is accepted by ICLR 2022. <be>
