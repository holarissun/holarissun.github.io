---
layout: archive
title: "News!"
permalink: /news/
author_profile: true
redirect_from:
  - /news
---


ðŸ‡¸ðŸ‡¬ (2025.04) I'll attend ICLR 2025 in-person. 

ðŸ‡ºðŸ‡¸ (2025.03) Guest lecture on [**Inverse RL Meets LLMs**](https://sites.google.com/view/irl-llm) at the UCLA Reinforcement Learning course. <br>

ðŸ‡ºðŸ‡¸ (2025.02) Attending AAAI 2025 to run the [**Tutorial: Inverse RL Meets LLMs**](https://sites.google.com/view/irl-llm). Thanks for joining us in Philadelphia! [Slide](https://github.com/holarissun/InverseRLmeetsLLMs/blob/main/IRLxLLMs_Feb25.pdf). <br>

ðŸ“„ (2025.02) Our Reward Model Paper [Part IV: Multi-Objective and Personalized Alignment with PCA](https://arxiv.org/abs/2502.13131) is online. <br> 

ðŸ“„ (2025.02) Our Reward Model Paper [Part III: Infrastructure for Reproducible Reward Model Research](https://arxiv.org/pdf/2502.04357) os online. <br> 

ðŸ“„ (2025.02) Our Reward Model Paper [Part II: Active Reward Modeling](https://arxiv.org/pdf/2502.04354) is online. <br> 

ðŸ“„ (2025.01) Our Reward Model Paper [Part I: Foundation, Theory, and Alternatives](https://arxiv.org/pdf/2411.04991) is accepted by ICLR as an **<mark>Oral ðŸŽ‰</mark>**. It is an amazing experience to co-lead this paper wity [Yunyi](https://yunyishen.github.io/) and advised by [Jef](https://savior287.github.io/JFT-webpage/).

ðŸ‡¦ðŸ‡¹ (2024.12) We will run the [**Tutorial: Inverse RL Meets LLMs**](https://sites.google.com/view/irl-llm) at ACL-2025, see you at Vienna!<be>

ðŸ‡¬ðŸ‡§ (2024.10) New talk on **Inverse RL Meets LLMs** at the vdsLab2024 OpenHouse and UCLA Zhou Lab. [Slide](https://holarissun.github.io/files/IRL_LLM_Oct.pdf) is online<be>

ðŸ“„ (2024.09) Our [Data Centric Reward Modeling](https://openreview.net/forum?id=wg5y4AK6l7) paper is accepted by the Journal of Data-Centric Machine Learning Research (DMLR). <be>

ðŸ‡ºðŸ‡¸ (2024.08) [InverseRLignment](https://openreview.net/pdf/97e8ef1506b4477fd9dc41a76ea3257f65c66c5e.pdf) is presented at the RL beyond reward workshop (accepted with score 9) at the 1-st RLConference, it **builds reward models from SFT data**.. <be>

ðŸ“„ (2024.05) Our [RLHF with Dense Reward](https://arxiv.org/pdf/2402.00782.pdf) paper is accepted by ICML 2024. <be>

ðŸ‡¬ðŸ‡§ (2024.03) **Prompt-OIRL** and **RATP** are featured at the [Inspiration Exchange](https://www.vanderschaar-lab.com/engagement-sessions/inspiration-exchange/), recording is <a href="https://www.youtube.com/watch?v=NYYYbQ_EN30&ab_channel=vanderSchaarLab"> online </a>. <be>

ðŸ‡¦ðŸ‡¹ (2024.01) **1 RL + LLM Reasoning paper** is accepted by ICLR 2024! [Prompt-OIRL](https://arxiv.org/pdf/2309.06553.pdf) uses Inverse RL to evaluate and optimize prompts for **Math Reasoning**.<be>

ðŸ‡ºðŸ‡¸ (2024.01) Invited talk on **RLHF** at the [Intuit AI Research Forum](https://www.intuit.com/technology/). <a href="https://holarissun.github.io/files/RLHF_Dec.pdf"> slide </a> <be>

ðŸ‡¨ðŸ‡³ (2023.12) Invited talk on **RLHF** at the [Likelihood Lab](http://www.maxlikelihood.cn/) <a href="https://holarissun.github.io/files/RLHF_Dec.pdf"> slide </a> <be>

ðŸ‡¨ðŸ‡³ (2023.11) Invited talk on **RLHF** at the [CoAI group, THU.](https://huggingface.co/thu-coai). <a href="https://holarissun.github.io/files/RLHF_Nov.pdf"> slide  </a> <be>

ðŸ“„ (2023.10) [Prompt-OIRL](https://arxiv.org/pdf/2309.06553.pdf) is selected as an **<mark>oral presentation ðŸŽ‰</mark>** at the NeurIPS 2023 ENLSP workshop!<be>

ðŸ“„ (2023.10) I wrote <a href="https://arxiv.org/abs/2310.06147">an article </a> to share my thoughts as an RL researcher in the Era of LLMs. <be>

ðŸ“„ (2023.09) **2 papers** on [Interpretable Offline RL](https://arxiv.org/abs/2310.07747) and [Interpretable Uncertainty Quantification](https://arxiv.org/abs/2207.05161) are accepted by NeurIPS 2023. <br>

ðŸ‡¨ðŸ‡³ (2023.9) Invited talk on **"Reinforcement Learning in the Era of LLMs"** at [Kuaishou Research](https://usrdc.kuaishou.com/). <a href="https://holarissun.github.io/files/RLHF_Kuai_final.pdf"> slide is online </a>  <be>

ðŸ“„ (2023.2) **2 papers** are accepted by AISTATS 2023. <be>

ðŸ‡®ðŸ‡ª (2022.11) Invited talk on value-based DRL at HW Cloud Research. <a href="https://sites.google.com/view/rewardshaping"> slide is online </a>  <be>

ðŸ“„ (2022.9) **1 paper** on [Value-Based DeepRL](https://proceedings.neurips.cc/paper_files/paper/2022/file/f600d1a3f6a63f782680031f3ce241a7-Paper-Conference.pdf) is accepted by NeurIPS 2022. 2 papers are presented at the FMDM workshop, and 2 papers are presented at the DeepRL workshop. <be>

ðŸ“„ (2022.1) **1 paper** on [Offline GCRL](https://arxiv.org/abs/2202.04478) is accepted by ICLR 2022. <be>
