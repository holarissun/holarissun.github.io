---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

ğŸš€ **Hi there!** I am Hao Sun, a final-year Ph.D. student at the University of Cambridge, supervised by <a href="https://www.vanderschaar-lab.com/prof-mihaela-van-der-schaar/" style="color: gray; text-decoration: none;">Prof. Mihaela van der Schaar</a>, working at the intersection of reinforcement learning (RL) and large language models (LLMs). During my M.Phil. study at <a href="https://mmlab.ie.cuhk.edu.hk/" style="color: gray; text-decoration: none;">MMLab@CUHK</a>, I was advised by <a href="http://dahua.site/" style="color: gray; text-decoration: none;">Prof. Dahua Lin</a> and <a href="https://boleizhou.github.io/" style="color: gray; text-decoration: none;">Prof. Bolei Zhou</a>. I hold a B.Sc. in Physics from the Yuanpei College at Peking University, and a B.Sc. from the Guanghua School of Management. My undergraduate thesis was supervised by <a href="https://zhouchenlin.github.io/" style="color: gray; text-decoration: none;">Prof. Zhouchen Lin</a>.

### <mark> I am seeking full-time research positions starting in 2025</mark>
<br>

Research
====
_**High-level research interests and motivations:**_ My research focuses on RL and LLM Alignment (also known as post-training). RL (and interacting with the environment) is the key toward super-human intelligence, and more powerful LLMs --- using RL as training framework and natural language as the interaction interface --- can significantly enhance the ability of humans to learn from machine intelligence. 

## Representative Works

**I am particularly proud of the following research works.** <br>
_For full publication list, please check my <a href="https://scholar.google.com/citations?hl=en&user=7ZNoHJkAAAAJ&view_op=list_works&sortby=pubdate" style="color: gray; text-decoration: none;">Google Scholar</a> page._



### - Large Langauge Model Alignment (Since 2023)
- **[Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL](https://arxiv.org/pdf/2309.06553)**  
  *ICLR 2024, also NeurIPS 2023 ENLSP Workshop as **<span style="background-color: #f0f0f0; color: #333;">Oral</span>** Presentation*  
  **Hao Sun**, Alihan HÃ¼yÃ¼k, jMihaela van der Schaar  
    - We studied the large language model inference-time optimization on mathematical reasoning tasks.
    - We highlighted the importance of reward models in **<span style="background-color: #f0f0f0; color: #333;">LLM inference-time optimization for math</span>**, which has been recognized by the community as an important topic since late 2024 (a year after the paper has been finished).

- **[Rethinking the Bradley-Terry Models in Preference-based Reward Modeling: Foundation, Theory, and its Alternatives](https://arxiv.org/pdf/2411.04991)**  
  *ICLR 2025 **<span style="background-color: #f0f0f0; color: #333;">Oral (Top 1.2%)</span>***  
  **Hao Sun<sup>*</sup>**, Yunyi Shen<sup>*</sup>, Jean-Francois Ton ( &ast; denotes equal contribution)  
    - We studied the foundation of preference-based reinforcement learning from human feedback (RLHF) practices, answering the foundational question of why the Bradley-Terry model is a solid choice in RLHF.
    - **<span style="background-color: #f0f0f0; color: #333;">We justified and poineered the research direction of embedding-based reward modeling.</span>**
    - Our follow-up works further developed this agenda by studying active reward modeling in RLHF, efficient personalized alignment, and contributing computationally-efficient infrastructure to the research community.

### - Reinforcement Learning (Since 2018)
- **[Policy Continuation with Hindsight Inverse Dynamics](https://proceedings.neurips.cc/paper_files/paper/2019/file/3891b14b5d8cce2fdd8dcdb4ded28f6d-Paper.pdf)**  
  *NeurIPS 2019 **<span style="background-color: #f0f0f0; color: #333;">Spotlight (Top 2.4%)</span>***  
  **Hao Sun**, Zhizhong Li, Dahua Lin, Bolei Zhou  
    - We innovated the first self-imitation-learning paradigm of multi-goal RL.
    - Our paper **<span style="background-color: #f0f0f0; color: #333;">pioneered the research field of supervised-learning-based goal-conditioned RL</span>**, and has been followed by the research works from [UC Berkeley Paper 1](https://openreview.net/forum?id=ByxoqJrtvr) [and Paper 2](https://arxiv.org/pdf/2011.08909) since 2021.
    - Our [follow-up work](https://openreview.net/pdf?id=R9jakCHb_1C) extended this idea into general continuous control settings.
    - Our [follow-up work](https://arxiv.org/pdf/2202.04478) published at ICLR 2022 connects this idea with offline-RL

- **[Exploiting Reward Shifting in Value-based Deep RL](https://openreview.net/pdf?id=iCxRsZcVVAH)**  
  *NeurIPS 2022*  
  **Hao Sun**, Lei Han, Rui Yang, Xiaoteng Ma, Bolei Zhou  
    - We added **<span style="background-color: #f0f0f0; color: #333;">new insights to the fundamental dilemma of exploration and exploitation trade-offs</span>** through reward shifting.
    - This line of research has been followed by [research from Professor Richard Sutton's group](https://arxiv.org/pdf/2405.09999) since 2024.
    - This method has been widely verified in RL applications such as <a href="https://www.ijcai.org/proceedings/2023/0514.pdf" style="color: gray; text-decoration: none;">offline RL (ICJAI'23))</a>, <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10801909" style="color: gray; text-decoration: none;">robotics locomotion (IROS'24)</a>, <a href="https://openreview.net/pdf?id=AS0S1flXxR" style="color: gray; text-decoration: none;">optimistic initialization (AAMAS'24)</a> , <a href="https://arxiv.org/pdf/2411.11099" style="color: gray; text-decoration: none;">multi-agent exploration (Recent preprint)</a>.


<br>


News!
======

ğŸ‡¸ğŸ‡¬ (2025.04) I'll attend ICLR 2025 in-person. 

ğŸ‡ºğŸ‡¸ (2025.03) Guest lecture on [**Inverse RL Meets LLMs**](https://sites.google.com/view/irl-llm) at the UCLA Reinforcement Learning course. <br>

ğŸ‡ºğŸ‡¸ (2025.02) Attending AAAI 2025 to run the [**Tutorial: Inverse RL Meets LLMs**](https://sites.google.com/view/irl-llm). Thanks for joining us in Philadelphia! [Slide](https://github.com/holarissun/InverseRLmeetsLLMs/blob/main/IRLxLLMs_Feb25.pdf). <br>

ğŸ“„ (2025.02) Our Reward Model Paper [Part IV: Multi-Objective and Personalized Alignment with PCA](https://arxiv.org/abs/2502.13131) is online. <br> 

ğŸ“„ (2025.02) Our Reward Model Paper [Part III: Infrastructure for Reproducible Reward Model Research](https://arxiv.org/pdf/2502.04357) os online. <br> 

ğŸ“„ (2025.02) Our Reward Model Paper [Part II: Active Reward Modeling](https://arxiv.org/pdf/2502.04354) is online. <br> 

ğŸ“„ (2025.01) Our Reward Model Paper [Part I: Foundation, Theory, and Alternatives](https://arxiv.org/pdf/2411.04991) is accepted by ICLR as an **<mark>Oral ğŸ‰</mark>**. It is an amazing experience to co-lead this paper wity [Yunyi](https://yunyishen.github.io/) and advised by [Jef](https://savior287.github.io/JFT-webpage/).

ğŸ‡¦ğŸ‡¹ (2024.12) We will run the [**Tutorial: Inverse RL Meets LLMs**](https://sites.google.com/view/irl-llm) at ACL-2025, see you at Vienna!<be>

ğŸ‡¬ğŸ‡§ (2024.10) New talk on **Inverse RL Meets LLMs** at the vdsLab2024 OpenHouse and UCLA Zhou Lab. [Slide](https://holarissun.github.io/files/IRL_LLM_Oct.pdf) is online<be>

ğŸ“„ (2024.09) Our [Data Centric Reward Modeling](https://openreview.net/forum?id=wg5y4AK6l7) paper is accepted by the Journal of Data-Centric Machine Learning Research (DMLR). <be>

ğŸ‡ºğŸ‡¸ (2024.08) [InverseRLignment](https://openreview.net/pdf/97e8ef1506b4477fd9dc41a76ea3257f65c66c5e.pdf) is presented at the RL beyond reward workshop (accepted with score 9) at the 1-st RLConference, it **builds reward models from SFT data**.. <be>

ğŸ“„ (2024.05) Our [RLHF with Dense Reward](https://arxiv.org/pdf/2402.00782.pdf) paper is accepted by ICML 2024. <be>

ğŸ‡¬ğŸ‡§ (2024.03) **Prompt-OIRL** and **RATP** are featured at the [Inspiration Exchange](https://www.vanderschaar-lab.com/engagement-sessions/inspiration-exchange/), recording is <a href="https://www.youtube.com/watch?v=NYYYbQ_EN30&ab_channel=vanderSchaarLab"> online </a>. <be>

ğŸ‡¦ğŸ‡¹ (2024.01) **1 RL + LLM Reasoning paper** is accepted by ICLR 2024! [Prompt-OIRL](https://arxiv.org/pdf/2309.06553.pdf) uses Inverse RL to evaluate and optimize prompts for **Math Reasoning**.<be>

ğŸ‡ºğŸ‡¸ (2024.01) Invited talk on **RLHF** at the [Intuit AI Research Forum](https://www.intuit.com/technology/). <a href="https://holarissun.github.io/files/RLHF_Dec.pdf"> slide </a> <be>

ğŸ‡¨ğŸ‡³ (2023.12) Invited talk on **RLHF** at the [Likelihood Lab](http://www.maxlikelihood.cn/) <a href="https://holarissun.github.io/files/RLHF_Dec.pdf"> slide </a> <be>

ğŸ‡¨ğŸ‡³ (2023.11) Invited talk on **RLHF** at the [CoAI group, THU.](https://huggingface.co/thu-coai). <a href="https://holarissun.github.io/files/RLHF_Nov.pdf"> slide  </a> <be>

ğŸ“„ (2023.10) [Prompt-OIRL](https://arxiv.org/pdf/2309.06553.pdf) is selected as an **<mark>oral presentation ğŸ‰</mark>** at the NeurIPS 2023 ENLSP workshop!<be>

ğŸ“„ (2023.10) I wrote <a href="https://arxiv.org/abs/2310.06147">an article </a> to share my thoughts as an RL researcher in the Era of LLMs. <be>

ğŸ“„ (2023.09) **2 papers** on [Interpretable Offline RL](https://arxiv.org/abs/2310.07747) and [Interpretable Uncertainty Quantification](https://arxiv.org/abs/2207.05161) are accepted by NeurIPS 2023. <br>





<a href="https://clustrmaps.com/site/1bysk"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=RtOCs2DxbgCleb2bwL7ZaU9kONDpyPNXGY_Guo_CtaM&cl=ffffff" /></a>

<!--

Education
======
 <span style="font-weight: bold;"> ğŸ’ª Ph.D., van der Schaar Lab, University of Cambridge, Jun.2025 (expected)<br>
  </span>
  - Research Topic: Reality-Centric Deep Reinforcement Learning

  <span style="font-weight: bold;"> ğŸ“ M.Phil., MMLab, The Chinese University of Hong Kong, Sep.2021.<br>
  </span>
  - Thesis:
    <a href="https://github.com/2Groza/MPhil_Thesis/blob/main/MPhil_Thesis.pdf">Toward Practical Deep Reinforcement Learning: Sample-Efficient Self-Supervised Continuous Control</a><br>
  
  - Slide can be found at: 
    <a href="https://github.com/2Groza/MPhil_Thesis/blob/main/Toward%20Practical%20Reinforcement%20Learning.pptx">Slide</a><br>
  <p class="item_desc"></p>
  
  
<span style="font-weight: bold;"> ğŸ‘¨â€ğŸ“ B.Sc., School of Physics & Yuanpei College, Peking University, Jul.2018.<br>
</span>


I worked as an RA at the LCDM group@UIUC. I used to work on cosmology gravitational lensing in Prof.  and Ultracold atom during my undergrad research.
-->
