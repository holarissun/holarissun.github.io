---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

🚀 I am a penultimate year PhD student at the University of Cambridge, supervised by <a href="https://www.vanderschaar-lab.com/prof-mihaela-van-der-schaar/">Prof. Mihaela van der Schaar</a>. During my M.Phil. study at MMLab@CUHK, I was advised by <a href="http://dahua.me/">Prof. Dahua Lin</a> and <a href="http://bzhou.ie.cuhk.edu.hk/">Prof. Bolei Zhou</a>; I received my BSc in Physics from the Yuanpei Honor Program, at Peking University, and a BSc from the Guanghua School of Management, at Peking University. My undergrad thesis was advised by <a href="https://zhouchenlin.github.io/">Prof. Zhouchen Lin</a>.


🤖️ I believe **Reinforcement Learning** is a vital component of the solution for achieving AGI. My previous work on deep reinforcement learning is motivated by practical applications like robotics, healthcare, finance, and large language models. My research keywords during the past 4 years include:
- RL via Supervised Learning (2020-); Goal-Conditioned RL (2020-)
- Value-Based DRL (2021-); Offline RL (2021-); Optimism in Exploration (2021-); 
- **Uncertainty Quantification** (2022-); **Data-Centric Off-Policy Evaluation** (2022-); 
- **Interpretable RL** (2023-); **RL in Language Models.** (2023-)

🤝 I'm open to collaborations. Please drop me an email if you find my work interesting. Let us push RL closer to genuine general intelligence!




News
======
💬 (2023.11) I'm thrilled to share my thoughts on **RLHF** with the CoAI group, THU. <be> <a href="https://holarissun.github.io/files/RLHF_Nov.pdf"> slide is online </a> <br>
📄 (2023.10) Prompt-OIRL is selected as an **oral presentation** at the NeurIPS 2023 ENLSP workshop! <br>
📄 (2023.10) I wrote <a href="https://arxiv.org/abs/2310.06147">an article </a> on **RLHF** to share my thoughts as an RL researcher in the Era of LLMs. <br>
📄 (2023.9) Our work **Prompt-OIRL** on offline prompt evaluation and optimization using IRL is <a href="https://arxiv.org/pdf/2309.06553.pdf">online. </a><br>
📄 (2023.9) **2 papers** are accepted by NeurIPS 2023. I'm looking forward to the reunion in New Orleans! <br>
💬 (2023.9) I'm honored to share my experience and ideas with Kuaishou Research in a talk titled "Reinforcement Learning in the Era of LLMs". <be> <a href="https://holarissun.github.io/files/RLHF_Kuai_final.pdf"> slide is online </a>  <br>
📄 (2023.2) **2 papers** are accepted by AISTATS 2023. <br>
💬 (2022.11) I'm honored to share my experience and ideas with HW Cloud Research through a talk on value-based DRL. <be> <a href="https://sites.google.com/view/rewardshaping"> slide is online </a>  <br>
📄 (2022.9) **1 paper** is accepted by NeurIPS 2022. 2 papers are presented at the FMDM workshop, and 2 papers are presented at the DeepRL workshop. <br>
📄 (2022.1) **1 paper** is accepted by ICLR 2022. <br>


<!--

Education
======
 <span style="font-weight: bold;"> 💪 Ph.D., van der Schaar Lab, University of Cambridge, Jun.2025 (expected)<br>
  </span>
  - Research Topic: Reality-Centric Deep Reinforcement Learning

  <span style="font-weight: bold;"> 🎓 M.Phil., MMLab, The Chinese University of Hong Kong, Sep.2021.<br>
  </span>
  - Thesis:
    <a href="https://github.com/2Groza/MPhil_Thesis/blob/main/MPhil_Thesis.pdf">Toward Practical Deep Reinforcement Learning: Sample-Efficient Self-Supervised Continuous Control</a><br>
  
  - Slide can be found at: 
    <a href="https://github.com/2Groza/MPhil_Thesis/blob/main/Toward%20Practical%20Reinforcement%20Learning.pptx">Slide</a><br>
  <p class="item_desc"></p>
  
  
<span style="font-weight: bold;"> 👨‍🎓 B.Sc., School of Physics & Yuanpei College, Peking University, Jul.2018.<br>
</span>


I worked as an RA at the LCDM group@UIUC. I used to work on cosmology gravitational lensing in Prof.  and Ultracold atom during my undergrad research.
-->
