---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

ğŸš€ I am a PhD student at the University of Cambridge, supervised by <a href="https://www.vanderschaar-lab.com/prof-mihaela-van-der-schaar/">Prof. Mihaela van der Schaar</a>. During my M.Phil. study at MMLab@CUHK, I was advised by <a href="http://dahua.me/">Prof. Dahua Lin</a> and <a href="http://bzhou.ie.cuhk.edu.hk/">Prof. Bolei Zhou</a>; I received my BSc in Physics from the Yuanpei Honor Program, at Peking University, and a BSc from the Guanghua School of Management, at Peking University. My undergrad thesis was advised by <a href="https://zhouchenlin.github.io/">Prof. Zhouchen Lin</a>.
I worked as an RA at the LCDM group@UIUC.

ğŸ¤–ï¸ I believe **Reinforcement Learning** is a vital part of the solution for AGI. My previous work on deep reinforcement learning is motivated by practical applications like robotics, healthcare, finance, and large language models. My research keywords during the past 4 years include:
- RL via Supervised Learning (2020-); Goal-Conditioned RL (2020-)
- Value-Based DRL (2021-); Offline RL (2021-); Optimism in Exploration (2021-); 
- **Uncertainty Quantification** (2022-); **Data-Centric Off-Policy Evaluation** (2022-); 
- **Interpretable RL**(2023-); **RL in Language Models.** (2023-)

ğŸ¤ I'm open to potential collaborations. Please kindly drop me an email for a chat. And let us push RL closer to genuine general intelligence.


Education
======
 <span style="font-weight: bold;"> ğŸ’ª Ph.D., van der Schaar Lab, University of Cambridge, Jun.2025 (expected)<br>
  </span>
  - Research Topic: Reality-Centric Deep Reinforcement Learning

  <span style="font-weight: bold;"> ğŸ“ M.Phil., MMLab, The Chinese University of Hong Kong, Sep.2021.<br>
  </span>
  - Thesis:
    <a href="https://github.com/2Groza/MPhil_Thesis/blob/main/MPhil_Thesis.pdf">Toward Practical Deep Reinforcement Learning: Sample-Efficient Self-Supervised Continuous Control</a><br>
  
  - Slide can be found at: 
    <a href="https://github.com/2Groza/MPhil_Thesis/blob/main/Toward%20Practical%20Reinforcement%20Learning.pptx">Slide</a><br>
  <p class="item_desc"></p>
  
  
<span style="font-weight: bold;"> ğŸ‘¨â€ğŸ“ B.Sc., School of Physics & Yuanpei College, Peking University, Jul.2018.<br>
</span>

News
======
ğŸ¤ _**I'm looking for internships for 2024. Please drop me an email if you find my research interesting.**_

ğŸ™Œ (2023.9) 2 papers are accepted by NeurIPS 2023. <br>
ğŸ”¥ (2023.9) I'm honored to share my experience and ideas with Kuaishou Research in a talk titled "Reinforcement Learning in the Era of LLMs". <be> <a href="https://holarissun.github.io/files/RLHF_Kuai_final.pdf"> slide is online </a>  <br>
ğŸ”¥ (2023.9) Our work on offline prompt evaluation and optimization using inverse RL is <a href="https://arxiv.org/pdf/2309.06553.pdf">now online. </a><br>
ğŸ“° (2023.2) 2 papers are accepted by AISTATS 2023. <br>
ğŸ“° (2022.9) 1 paper is accepted by NeurIPS 2022. 2 papers are presented at the FMDM workshop, and 2 papers are presented at the DeepRL workshop. <br>
ğŸ“° (2022.9) 1 paper is presented at the ICML 2022 DFUQ workshop. <br>
ğŸ“° (2022.1) 1 paper is accepted by ICLR 2022. <br>


<!--
A data-driven personal website
======
Like many other Jekyll-based GitHub Pages templates, academicpages makes you separate the website's content from its form. The content & metadata of your website are in structured markdown files, while various other files constitute the theme, specifying how to transform that content & metadata into HTML pages. You keep these various markdown (.md), YAML (.yml), HTML, and CSS files in a public GitHub repository. Each time you commit and push an update to the repository, the [GitHub pages](https://pages.github.com/) service creates static HTML pages based on these files, which are hosted on GitHub's servers free of charge. -->

