---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

ğŸš€ **Hi there!** I am Hao Sun, a final-year Ph.D. student at the University of Cambridge, supervised by [Prof. Mihaela van der Schaar](https://www.vanderschaar-lab.com/prof-mihaela-van-der-schaar/), working at the intersection of reinforcement learning (RL) and large language models (LLMs). During my M.Phil. study at [MMLab@CUHK](https://mmlab.ie.cuhk.edu.hk/), I was advised by [Prof. Dahua Lin](http://dahua.site/) and [Prof. Bolei Zhou](https://boleizhou.github.io/). I hold a B.Sc. in Physics from the Yuanpei College at Peking University, and a B.Sc. from the Guanghua School of Management. My undergraduate thesis was supervised by [Prof. Zhouchen Lin](https://zhouchenlin.github.io/).

<mark> I am seeking research scientist and academic positions starting in 2025. </mark>


News!
======
ğŸ“„ (2025.01) Two Papers on reward modeling will be available soon (Reward Model: Part II and Part III) <br>
ğŸ“„ (2025.01) Our [Reward Model Paper (Part I): Foundation](https://arxiv.org/pdf/2411.04991) is accepted by ICLR! <br>
ğŸ’¬ (2024.12) Our **[Tutorial: Inverse RL Meets LLMs](https://sites.google.com/view/irl-llm)** has been accepted by ACL-2025, see you at Vienna!<br>
ğŸ’¬ (2024.10) Our **[Tutorial: Inverse RL Meets LLMs](https://sites.google.com/view/irl-llm)** has been accepted by AAAI-2025 --- Join us in Philadelphia and Vienna --- and let us explore the potential of Inverse RL in the era of LLMs! <br>
ğŸ’¬ (2024.10) New talk on **Inverse RL Meets LLMs** at the vdsLab2024 OpenHouse and UCLA Zhou Lab. This talk summarizes our efforts in using **IRL for better Prompting, Fine-Tuning, and Inference-Time Optimization.** [Slide](https://holarissun.github.io/files/IRL_LLM_Oct.pdf) is online<br>
ğŸ“„ (2024.09) Our [Data Centric Reward Modeling](https://openreview.net/forum?id=wg5y4AK6l7) paper is accepted by the Journal of Data-Centric Machine Learning Research (DMLR). <br>
ğŸ“„ (2024.08) [InverseRLignment](https://openreview.net/pdf/97e8ef1506b4477fd9dc41a76ea3257f65c66c5e.pdf) is presented at the RL beyond reward workshop (accepted with score 9) at the 1-st RLC. <br>
ğŸ“„ (2024.05) [InverseRLignment](https://openreview.net/pdf/97e8ef1506b4477fd9dc41a76ea3257f65c66c5e.pdf) is online, it **builds reward models from SFT data**. <br>
ğŸ“„ (2024.05) Our [Dense Reward Model](https://arxiv.org/pdf/2402.00782.pdf) paper is accepted by ICML 2024. <br>
ğŸ“„ (2024.03) I wrote <a href="https://arxiv.org/abs/2403.12017">an article </a> arguing that **Supervised Fine Tuning is Inverse Reinforcement Learning**! <br>
ğŸ’¬ (2024.03) **Prompt-OIRL** and **RATP** are featured at the [Inspiration Exchange](https://www.vanderschaar-lab.com/engagement-sessions/inspiration-exchange/), recording is <a href="https://www.youtube.com/watch?v=NYYYbQ_EN30&ab_channel=vanderSchaarLab"> online </a>. <br>
ğŸ“„ (2024.02) **2 RL+LLM papers** are online! [ABC](https://arxiv.org/pdf/2402.00782.pdf) uses the attention mechanism to solve the credit assignment problem in RLHF; [RATP](https://arxiv.org/pdf/2402.07812.pdf) uses MCTS to enhance the reasoning ability of LLMs with external documents<br>
ğŸ“„ (2024.01) **1 RL+LLM paper** is accepted by ICLR 2024! [Prompt-OIRL](https://arxiv.org/pdf/2309.06553.pdf) uses Inverse RL to Evaluate and Optimize Prompts for Reasoning.<br>
ğŸ’¬ (2024.01) Invited talk on **RLHF** at the [Intuit AI Research Forum](https://www.intuit.com/technology/). <a href="https://holarissun.github.io/files/RLHF_Dec.pdf"> slide </a> <br>
ğŸ’¬ (2023.12) Invited talk on **RLHF** at the [Likelihood Lab](http://www.maxlikelihood.cn/) <a href="https://holarissun.github.io/files/RLHF_Dec.pdf"> slide </a> <br>
ğŸ’¬ (2023.11) Invited talk on **RLHF** at the [CoAI group, THU.](https://huggingface.co/thu-coai). <a href="https://holarissun.github.io/files/RLHF_Nov.pdf"> slide  </a> <br>
ğŸ“„ (2023.10) [Prompt-OIRL](https://arxiv.org/pdf/2309.06553.pdf) is selected as an **oral presentation** at the NeurIPS 2023 ENLSP workshop!<br>
ğŸ“„ (2023.10) I wrote <a href="https://arxiv.org/abs/2310.06147">an article </a> on **RLHF** to share my thoughts as an RL researcher in the Era of LLMs. <br>
ğŸ“„ (2023.9) **2 papers** on [Interpretable Offline RL](https://arxiv.org/abs/2310.07747) and [Interpretable Uncertainty Quantification](https://arxiv.org/abs/2207.05161) are accepted by NeurIPS 2023. <br>




<a href="https://clustrmaps.com/site/1bysk"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=RtOCs2DxbgCleb2bwL7ZaU9kONDpyPNXGY_Guo_CtaM&cl=ffffff" /></a>

<!--

Education
======
 <span style="font-weight: bold;"> ğŸ’ª Ph.D., van der Schaar Lab, University of Cambridge, Jun.2025 (expected)<br>
  </span>
  - Research Topic: Reality-Centric Deep Reinforcement Learning

  <span style="font-weight: bold;"> ğŸ“ M.Phil., MMLab, The Chinese University of Hong Kong, Sep.2021.<br>
  </span>
  - Thesis:
    <a href="https://github.com/2Groza/MPhil_Thesis/blob/main/MPhil_Thesis.pdf">Toward Practical Deep Reinforcement Learning: Sample-Efficient Self-Supervised Continuous Control</a><br>
  
  - Slide can be found at: 
    <a href="https://github.com/2Groza/MPhil_Thesis/blob/main/Toward%20Practical%20Reinforcement%20Learning.pptx">Slide</a><br>
  <p class="item_desc"></p>
  
  
<span style="font-weight: bold;"> ğŸ‘¨â€ğŸ“ B.Sc., School of Physics & Yuanpei College, Peking University, Jul.2018.<br>
</span>


I worked as an RA at the LCDM group@UIUC. I used to work on cosmology gravitational lensing in Prof.  and Ultracold atom during my undergrad research.
-->
